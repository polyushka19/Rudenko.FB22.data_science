{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Імпорт бібліотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import urllib\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "folder_path = 'download'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завантаження даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(province_id, year1=1981, year2=2024):\n",
    "    url = f\"https://www.star.nesdis.noaa.gov/smcd/emb/vci/VH/get_TS_admin.php?country=UKR&provinceID={province_id}&year1={year1}&year2={year2}&type=Mean\"\n",
    "    response = urllib.request.urlopen(url)\n",
    "    \n",
    "    if response.status == 200:  \n",
    "        current_datetime = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "        filename = f'vhi_id__{province_id}__{current_datetime}.csv'\n",
    "        with open(f'download/{filename}', 'wb') as out: \n",
    "            out.write(response.read())\n",
    "        print(f\"VHI is downloaded for province ID {province_id} into {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download data for province ID {province_id}. HTTP status code: {response.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VHI is downloaded for province ID 1 into vhi_id__1__2024-05-27_11-49.csv\n",
      "VHI is downloaded for province ID 2 into vhi_id__2__2024-05-27_11-50.csv\n",
      "VHI is downloaded for province ID 3 into vhi_id__3__2024-05-27_11-51.csv\n",
      "VHI is downloaded for province ID 4 into vhi_id__4__2024-05-27_11-52.csv\n",
      "VHI is downloaded for province ID 5 into vhi_id__5__2024-05-27_11-52.csv\n",
      "VHI is downloaded for province ID 6 into vhi_id__6__2024-05-27_11-53.csv\n",
      "VHI is downloaded for province ID 7 into vhi_id__7__2024-05-27_11-54.csv\n",
      "VHI is downloaded for province ID 8 into vhi_id__8__2024-05-27_11-54.csv\n",
      "VHI is downloaded for province ID 9 into vhi_id__9__2024-05-27_11-55.csv\n",
      "VHI is downloaded for province ID 10 into vhi_id__10__2024-05-27_11-56.csv\n",
      "VHI is downloaded for province ID 11 into vhi_id__11__2024-05-27_11-57.csv\n",
      "VHI is downloaded for province ID 12 into vhi_id__12__2024-05-27_11-58.csv\n",
      "VHI is downloaded for province ID 13 into vhi_id__13__2024-05-27_11-59.csv\n",
      "VHI is downloaded for province ID 14 into vhi_id__14__2024-05-27_12-00.csv\n",
      "VHI is downloaded for province ID 15 into vhi_id__15__2024-05-27_12-01.csv\n",
      "VHI is downloaded for province ID 16 into vhi_id__16__2024-05-27_12-01.csv\n",
      "VHI is downloaded for province ID 17 into vhi_id__17__2024-05-27_12-02.csv\n",
      "VHI is downloaded for province ID 18 into vhi_id__18__2024-05-27_12-03.csv\n",
      "VHI is downloaded for province ID 19 into vhi_id__19__2024-05-27_12-04.csv\n",
      "VHI is downloaded for province ID 20 into vhi_id__20__2024-05-27_12-05.csv\n",
      "VHI is downloaded for province ID 21 into vhi_id__21__2024-05-27_12-06.csv\n",
      "VHI is downloaded for province ID 22 into vhi_id__22__2024-05-27_12-07.csv\n",
      "VHI is downloaded for province ID 23 into vhi_id__23__2024-05-27_12-08.csv\n",
      "VHI is downloaded for province ID 24 into vhi_id__24__2024-05-27_12-09.csv\n",
      "VHI is downloaded for province ID 25 into vhi_id__25__2024-05-27_12-09.csv\n",
      "VHI is downloaded for province ID 26 into vhi_id__26__2024-05-27_12-10.csv\n",
      "VHI is downloaded for province ID 27 into vhi_id__27__2024-05-27_12-11.csv\n",
      "The loading of test structural data was successful\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 28):\n",
    "    download_data(i)\n",
    "print(\"The loading of test structural data was successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_id_name = {\n",
    "    1: \"Cherkasy\",\n",
    "    2: \"Chernihiv\",\n",
    "    3: \"Chernivtsi\",\n",
    "    4: \"Crimea\",\n",
    "    5: \"Dnipropetrovs'k\",\n",
    "    6: \"Donets'k\",\n",
    "    7: \"Ivano-Frankivs'k\",\n",
    "    8: \"Kharkiv\",\n",
    "    9: \"Kherson\",\n",
    "    10: \"Khmel'nyts'kyy\",\n",
    "    11: \"Kiev\",\n",
    "    12: \"Kiev City\",\n",
    "    13: \"Kirovohrad\",\n",
    "    14: \"Luhans'k\",\n",
    "    15: \"L'viv\",\n",
    "    16: \"Mykolayiv\",\n",
    "    17: \"Odessa\",\n",
    "    18: \"Poltava\",\n",
    "    19: \"Rivne\",\n",
    "    20: \"Sevastopol\",\n",
    "    21: \"Sumy\",\n",
    "    22: \"Ternopil'\",\n",
    "    23: \"Transcarpathia\",\n",
    "    24: \"Vinnytsya\",\n",
    "    25: \"Volyn\",\n",
    "    26: \"Zaporizhzhya\",\n",
    "    27: \"Zhytomyr\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "def create_data_frame(folder_path1):\n",
    "\n",
    "    csv_files = glob.glob(folder_path1 + \"/*.csv\")\n",
    "\n",
    "    headers = ['Year', 'Week', 'SMN', 'SMT', 'VCI', 'TCI', 'VHI', 'empty']\n",
    "    frames = []\n",
    "\n",
    "    for file in csv_files:\n",
    "        region_id1 = int(file.split('__')[1]) \n",
    "        df = pd.read_csv(file, header=1, names=headers)  \n",
    "        df.at[0, 'Year'] =  df.at[0, 'Year'][9:]\n",
    "        df=df.drop(df.index[-1])\n",
    "        df = df.drop(df.loc[df['VHI'] == -1].index)\n",
    "        df = df.drop('empty', axis=1)\n",
    "        df.insert(0, 'region_id', region_id1, True)\n",
    "        frames.append(df)\n",
    "        \n",
    "    result = pd.concat(frames).drop_duplicates().reset_index(drop=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       region_id  Year  Week    SMN     SMT    VCI    TCI    VHI\n",
      "0             10  1982   1.0  0.059  258.24  51.11  48.78  49.95\n",
      "1             10  1982   2.0  0.063  261.53  55.89  38.20  47.04\n",
      "2             10  1982   3.0  0.063  263.45  57.30  32.69  44.99\n",
      "3             10  1982   4.0  0.061  265.10  53.96  28.62  41.29\n",
      "4             10  1982   5.0  0.058  266.42  46.87  28.57  37.72\n",
      "...          ...   ...   ...    ...     ...    ...    ...    ...\n",
      "58153          9  2024  16.0  0.287  296.47  71.74  10.09  40.92\n",
      "58154          9  2024  17.0  0.307  297.60  70.66  19.63  45.17\n",
      "58155          9  2024  18.0  0.323  298.29  69.48  29.98  49.76\n",
      "58156          9  2024  19.0  0.337  298.79  67.27  40.40  53.88\n",
      "58157          9  2024  20.0  0.342  299.11  63.62  48.00  55.84\n",
      "\n",
      "[58158 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "result_df = create_data_frame('download')\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_year_analysis(region_id, years=(\"1982\", \"2024\")):\n",
    "    result_df['Year'] = pd.to_numeric(result_df['Year'])\n",
    "\n",
    "    df2 = result_df[(result_df[\"Year\"].between(int(years[0]), int(years[1]))) & (result_df['region_id'] == region_id)]\n",
    "    region_name = reg_id_name[region_id]\n",
    "    vhi_max_reg = df2[\"VHI\"].max()\n",
    "    vhi_min_reg = df2[\"VHI\"].min()\n",
    "    print(f\"[+] {region_name}: min {vhi_min_reg} max {vhi_max_reg}\")\n",
    "    # print(df2.head())  # Для перегляду перших кількох рядків фільтрованого датафрейму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Cherkasy: min 10.68 max 83.7\n",
      "[+] Chernihiv: min 15.17 max 80.65\n",
      "[+] Chernivtsi: min 15.16 max 72.19\n",
      "[+] Crimea: min 13.28 max 90.96\n",
      "[+] Dnipropetrovs'k: min 17.58 max 93.17\n",
      "[+] Donets'k: min 6.26 max 96.18\n",
      "[+] Ivano-Frankivs'k: min 18.98 max 73.35\n",
      "[+] Kharkiv: min 9.36 max 91.42\n",
      "[+] Kherson: min 12.23 max 90.61\n",
      "[+] Khmel'nyts'kyy: min 18.41 max 79.4\n",
      "[+] Kiev: min 10.6 max 80.88\n",
      "[+] Kiev City: min 6.49 max 76.84\n",
      "[+] Kirovohrad: min 16.36 max 84.52\n",
      "[+] Luhans'k: min 12.45 max 90.32\n",
      "[+] L'viv: min 18.31 max 69.96\n",
      "[+] Mykolayiv: min 5.94 max 92.31\n",
      "[+] Odessa: min 5.52 max 89.14\n",
      "[+] Poltava: min 15.68 max 85.14\n",
      "[+] Rivne: min 20.7 max 77.45\n",
      "[+] Sevastopol: min 8.14 max 76.94\n",
      "[+] Sumy: min 16.37 max 79.84\n",
      "[+] Ternopil': min 20.63 max 77.71\n",
      "[+] Transcarpathia: min 18.0 max 72.59\n",
      "[+] Vinnytsya: min 11.25 max 82.64\n",
      "[+] Volyn: min 11.91 max 78.32\n",
      "[+] Zaporizhzhya: min 10.88 max 96.69\n",
      "[+] Zhytomyr: min 19.48 max 77.57\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 28):\n",
    "    region_year_analysis(region_id=i, years=(\"1985\", \"2010\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drought_years_analysis(df, years_range=(\"1981\", \"2024\"), threshold_extreme=15, threshold_moderate=(15, 35), percent_threshold=20):\n",
    "    extreme_drought_years = []\n",
    "    moderate_drought_years = []\n",
    "\n",
    "    total_regions = len(set(df['region_id']))\n",
    "    min_regions_affected = (percent_threshold / 100) * total_regions\n",
    "\n",
    "    for year in range(int(years_range[0]), int(years_range[1]) + 1):\n",
    "        yearly_data = df[df['Year'] == year]\n",
    "        extreme_drought_count = len(yearly_data[yearly_data['VHI'] < threshold_extreme])\n",
    "        moderate_drought_count = len(yearly_data[(yearly_data['VHI'] >= threshold_moderate[0]) & (yearly_data['VHI'] <= threshold_moderate[1])])\n",
    "\n",
    "        if extreme_drought_count >= min_regions_affected:\n",
    "            extreme_drought_years.append(year)\n",
    "        if moderate_drought_count >= min_regions_affected:\n",
    "            moderate_drought_years.append(year)\n",
    "\n",
    "    return extreme_drought_years, moderate_drought_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_regions = [3, 5, 12]  \n",
    "selected_years = (\"2005\", \"2015\")\n",
    "# vhi_for_regions_and_years(selected_regions, selected_years)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_drought_years, moderate_drought_years = drought_years_analysis(result_df, years_range=(\"2000\", \"2010\"))\n",
    "print(\"Роки з екстремальними посухами:\", extreme_drought_years)\n",
    "print(\"Роки з помірними посухами:\", moderate_drought_years)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
